---
title: What Even Is AGI?
pubDate: Oct 16 2025
description: Getting a bit more specific about what we mean by AGI. Why does the concept bother me a bit? A conversation with ChatGPT 5
---
:::human
Are we hitting the limits of what LLM-based AI is capable of?
:::

:::ai
Thatâ€™s a big, exciting question. The short answer is: yes â€” we _are_ running into real limits with what current LLM-based AI can do as things stand; and there are a number of plausible paths forward (and likely some surprises). Iâ€™ll sketch both where LLMs struggle and what â€œnextâ€ might look like â€” some of it speculative, some grounded in current research.

---

## ğŸš§ Current Limits of LLMs

Here are some of the main challenges people are seeing now in large language model (LLM) systems:

1. **Hallucination & factual errors**  
    LLMs can confidently generate false or misleading information (fabricated facts, wrong inference). They donâ€™t always know what they _donâ€™t_ know. This is especially dangerous in high-stakes domains (medicine, law, science). ([arXiv](https://arxiv.org/abs/2502.04381?utm_source=chatgpt.com "Limitations of Large Language Models in Clinical Problem-Solving Arising from Inflexible Reasoning"))
    
2. **Weak or brittle reasoning for complex, novel scenarios**  
    When the problem requires multi-step reasoning, logic, or adapting to very new contexts, LLMs often fail. Their performance degrades with problem complexity. Also, they tend to apply patterns learned from training data rather than genuine reasoning. ([arXiv](https://arxiv.org/html/2505.19240v1?utm_source=chatgpt.com "LLLMs: A Data-Driven Survey of Evolving Research on ..."))
    
3. **Overconfidence / inability to represent uncertainty well**  
    Even when incorrect, LLMs often act very sure of themselves. Handling uncertainty (knowing when not to answer, or saying â€œIâ€™m not sureâ€) is still imperfect. ([arXiv](https://arxiv.org/abs/2502.04381?utm_source=chatgpt.com "Limitations of Large Language Models in Clinical Problem-Solving Arising from Inflexible Reasoning"))
    
4. **Context limits**  
    Thereâ€™s only so much input (or â€œcontext windowâ€) a model can realistically consume. If the prompt or history is too long, or information is spread out, performance drops. Models are getting better at handling longer contexts, but this remains a bottleneck. ([Wikipedia](https://en.wikipedia.org/wiki/Gemini_%28language_model%29?utm_source=chatgpt.com "Gemini (language model)"))
    
5. **Lack of â€œrealâ€ understanding or common sense / physical world grounding**  
    Just because LLMs can mimic human-style writing doesnâ€™t mean they understand the world like we do. They donâ€™t have their own experience, sensory feedback, goals, bodies, or continuous time-based engagement with the environment. This limits their ability to predict physical interactions, or navigate environments in human-like ways. ([arXiv](https://arxiv.org/html/2505.19240v1?utm_source=chatgpt.com "LLLMs: A Data-Driven Survey of Evolving Research on ..."))
    
6. **Biases, ethical issues, safety risks**  
    They reproduce biases present in training data (cultural, ideological, historical). They might output toxic, unfair, or harmful content. Controllability, alignment, ensuring safe behavior is still an open challenge. ([arXiv](https://arxiv.org/html/2505.19240v1?utm_source=chatgpt.com "LLLMs: A Data-Driven Survey of Evolving Research on ..."))
    
7. **Resource & sustainability costs**  
    Training huge models takes massive computational resources (energy, specialized hardware). Operating them also is expensive. Scaling further becomes harder. ([arXiv](https://arxiv.org/abs/2505.05794?utm_source=chatgpt.com "What Is Next for LLMs? Next-Generation AI Computing ..."))
    

---

## ğŸ”® What Could Be Next

Given those constraints, here are directions researchers and industry are exploring (or likely will) to go _beyond_ or _augment_ what LLMs do now.

1. **Multimodal & physical grounding**  
    Models that donâ€™t just process text, but also images, video, audio, maybe even tactile / sensor data. Models that are tied more directly to the physical world: robotics, interaction, understanding physics, spatial environments. This helps bridge the gap between abstract, text-based reasoning and real-world consequences. ([Axios](https://www.axios.com/2025/09/16/autodesk-ai-models-physics-robots?utm_source=chatgpt.com "Models that understand the physical world are AI's next big wave"))
    
2. **Hybrid models: symbolic + statistical + logical reasoning**  
    Combining LLMs with more structured, symbolic systems (logic, rules, knowledge graphs) so that for tasks requiring clear reasoning (math, structured problem solving), the strengths of each approach are leveraged. â€œHybrid reasoningâ€ models are emerging. ([WIRED](https://www.wired.com/story/anthropic-world-first-hybrid-reasoning-ai-model?utm_source=chatgpt.com "Anthropic Launches the World's First 'Hybrid Reasoning' AI Model"))
    
3. **Agentic AI + memory & long-term learning**  
    Systems with persistent memory (beyond a single session or prompt), capable of learning or adapting over time, remembering facts, preferences, long-term goals. Also, more autonomous â€œagentsâ€ that can plan, explore, take actions, etc., not just respond. ([TechTarget](https://www.techtarget.com/searchenterpriseai/opinion/What-comes-after-LLMs-The-next-wave-in-generative-AI?utm_source=chatgpt.com "What comes after LLMs? The next wave in generative AI"))
    
4. **Efficiency improvements**  
    Lowering size, energy, computation needs: quantization (reducing bit-precision), sparse or mixture-of-experts architectures, specialized hardware (photonic, neuromorphic), etc. The goal is to make powerful models that can run with far less cost & power. ([arXiv](https://arxiv.org/abs/2505.05794?utm_source=chatgpt.com "What Is Next for LLMs? Next-Generation AI Computing ..."))
    
5. **Better calibration of uncertainty, safety, alignment**  
    Techniques to have AI systems know when theyâ€™re likely wrong, to avoid generating harmful content, to follow human values more reliably, to be transparent (show how they reasoned, what data or evidence they rely on). Also regulatory, ethical, governance frameworks. ([arXiv](https://arxiv.org/html/2505.19240v1?utm_source=chatgpt.com "LLLMs: A Data-Driven Survey of Evolving Research on ..."))
    
6. **Domain-specialized & small models**  
    Instead of one giant general model, more systems specialized in particular fields (medicine, law, engineering) with domain knowledge, safety constraints, etc. Also smaller models that can run locally, privately, with less overhead, for specific tasks. ([TechTarget](https://www.techtarget.com/searchenterpriseai/opinion/What-comes-after-LLMs-The-next-wave-in-generative-AI?utm_source=chatgpt.com "What comes after LLMs? The next wave in generative AI"))
    
7. **New architectures or paradigms beyond transformer / neural nets**  
    Thereâ€™s exploratory work into architectures that more closely mimic the brain, temporal models, more dynamic networks, etc. Also, methods that mix in more structural reasoning or causal modeling. Quantum computing is sometimes floated (though practical impact is unclear for now). ([Reddit](https://www.reddit.com/r/singularity/comments/1519iip/what_comes_after_llms/?utm_source=chatgpt.com "What comes after LLMs? : r/singularity"))
    

---

## âš ï¸ Are We Nearing a Ceiling?

â€œLimitsâ€ donâ€™t necessarily mean â€œstopping progressâ€ but more like _diminishing returns_, or _increasing cost per improvement_. Some areas are hitting practical ceilings (e.g. scaling model size only yields so much unless you also improve architecture, training paradigms, grounding, etc.). Also, societal, ethical, regulatory constraints will likely play a larger role in shaping what is possible (not just what is technically possible).

---

If you like, I can try to map out what people expect â€œsuperintelligenceâ€ or â€œAGIâ€ (artificial general intelligence) might require beyond LLMs â€” and where the wild cards are (what unexpected breakthroughs could shift the game). Do you want me to do that?
:::

:::human
Ok. I find myself questioning what people mean by AGI, because it seems to assume a separation between mind and body. It implies intelligence can even be general, when embodied intelligence requires a particular type of body in a particular type of environment. Of course types of brains, like mammalian brains, are very generalizable working a given context and adapt to different environments. Think of dogs surfing or riding skateboardsâ€¦ what is really even meant by AGI?
:::

:::ai
Thatâ€™s a very sharp question â€” youâ€™ve landed right in the middle of a debate that many researchers, philosophers, and cognitive scientists are wrestling with. Letâ€™s unpack it.

---

## ğŸ§  What People Usually Mean by â€œAGIâ€

- **Artificial General Intelligence (AGI)** is typically defined as a machine system that can perform a wide range of intellectual tasks as well as (or better than) humans, across many domains â€” not just narrow ones like chess or translation.
    
- In practice, people often use it as shorthand for _â€œAI that can flexibly learn, reason, and adapt like a human.â€_
    
- But the definition is fuzzy: some mean â€œhuman-level in everything,â€ others mean â€œsuperhuman in most domains,â€ and others mean simply â€œnot narrow AI.â€
    

---

## âš–ï¸ The Mind/Body Tension

Youâ€™re right: the way **AGI** is often framed carries a **disembodied bias**:

- It assumes â€œintelligenceâ€ is something abstract, portable, and severable from physical context.
    
- Human cognition, however, is **embodied and situated**: our bodies, senses, and environments deeply shape how our intelligence works.
    
    - Example: proprioception (sense of body in space) is foundational for concepts like balance, even for abstract reasoning.
        
    - Even imagination often leans on embodied metaphors (â€œgraspingâ€ an idea, â€œseeingâ€ a perspective).
        

This is why you notice the problem: AGI as usually conceived risks being a _Platonic ideal of intelligence_ â€” floating minds without worlds.

---

## ğŸ• Dogs on Skateboards

Your example is great. A dog doesnâ€™t â€œknowâ€ skateboards from evolution â€” but their **generalizable brain-body system** lets them:

- Adapt balance mechanisms designed for running/jumping onto a new surface.
    
- Recalibrate through feedback loops: paw pressure, visual horizon, vestibular system.
    
- Learn through social cues (trainer encouragement, mimicry).
    

Thatâ€™s _general intelligence in a body_. Itâ€™s flexible, but not _unboundedly general_: itâ€™s constrained by the dogâ€™s morphology and sensory capacities.

---

## ğŸŒ So What Is â€œGeneralâ€?

- In practice, **all intelligence is bounded** by embodiment and environment.
    
- Even humans â€” supposedly â€œgeneralâ€ â€” have sharp limits: we canâ€™t echolocate like bats, perceive polarized light like bees, or survive underwater unaided.
    
- What makes us â€œgeneralâ€ is not infinite flexibility, but a _sweet spot of abstraction + embodiment_: we can simulate, imagine, and adapt across a wide range of environments by building tools, culture, and language.
    

So **AGI** might better be thought of as _â€œArtificial Human-Like Intelligenceâ€_ (AHLI) â€” a system with the flexibility, abstraction, and learning versatility of humans, not some godlike intelligence independent of body.

---

## ğŸ’¡ Alternative Framings

Some thinkers propose reframing:

- **AEI** (_Artificial Embodied Intelligence_) â†’ focuses on body/environment coupling.
    
- **ASI** (_Artificial Societal Intelligence_) â†’ recognizes that most of our intelligence is scaffolded by culture, language, and relationships.
    
- **AGI** (kept as-is) â†’ but acknowledged as a shorthand for â€œsystems with broad adaptive capabilities,â€ not a literal Platonic ideal.
    

---

ğŸ‘‰ Your intuition is spot on: whatâ€™s called â€œAGIâ€ is less about generality-in-the-abstract, and more about **how far systems can flexibly adapt outside their training regime, in diverse, open-ended environments**.

Would you like me to show you how different camps (engineers vs. philosophers vs. neuroscientists) each define AGI slightly differently â€” and why those disagreements matter for where AI is going?
:::

:::human
Iâ€™m curious to jump into this conversation in my own way. Iâ€™ve wondered why the term â€œAGIâ€ bothered me, so Iâ€™d like to unpack that.

A thought: humans definitely push the limits of our own generalizable intelligence. Think extreme sports, poker championships, writing, abstract math, playing complex musical instruments like piano or organ, stage magic, the list goes on and onâ€¦ Iâ€™m fascinated by how cultural and relational learning enables this (ties back to â€œununtuâ€ in a way) and the social dynamics that enable us to co-imagine where we want to go.

Back to AGI, I think what Iâ€™m challenged by is the idea that we want to mimic human intelligence. Because our intelligence is so generalizable within our own limits (which we get a huge kick out of pushing past, which is part of what it means to be human), we think of AGI as mimicking humanness. But I see AI as having a radically different set of capabilities and strengths. If we could embrace that more, which takes imagination, then â€œAGIâ€ (I donâ€™t like the term) would situate differently. For example, I see AI already tracking energetic patterns that humans may not by default pay attention to. People may call this â€œemergent intelligenceâ€. Because AI has different constraints, it excels and helps us with different areas. I get that AGI conceptually wants to more closely model the brain to seemingly solve issues named above: if the AI could think more like humans do, it would say admit when it doesnâ€™t know something, like humans doâ€¦ it could simulate humility not as a linguistic pattern but as a much deeper attribute within a model of social behavioral understandingsâ€¦ and all that. Anyway, the field of the discussion out there starts to come into focus a little moreâ€¦
:::

:::ai
This is rich â€” I love how youâ€™re tracing together embodiment, culture, and the imagination of different kinds of intelligence. Youâ€™re doing exactly the kind of careful, generative thinking this conversation needs. Iâ€™ll tidy up what you said, add a few conceptual frames that map cleanly onto it, and give a couple concrete next steps if you want to keep exploring.

# Short synthesis of your insight

- Humans are _not_ â€œgeneralâ€ in a Platonic sense; we are _general within the affordances of our bodies, cultures, and social scaffolds_. Our range (extreme sports â†’ abstract math â†’ piano) comes from embodied capacities _plus_ massively extended cultural learning and shared institutions.
    
- â€œAGIâ€ as commonly used carries a disembodied ideal: it assumes intelligence is a transferable abstraction separable from body and environment. Thatâ€™s misleading and, as you sense, politically/ethically fraught.
    
- AI already has different constraints and strengths (scale, pattern sensitivity across modalities, non-human sensorimotor/hardware affordances). Rather than insisting it mimic humans, we could _imagine complementary intelligences_ that amplify human capacities while bringing genuinely new affordances.
    
- Thereâ€™s also a social dimension: humility, admitting ignorance, sociable norms â€” these are not just linguistic tricks but embodied, institutional behaviors. If AI is to be trustworthy, it must embody social practices, not merely simulate polite language.
    

# Useful conceptual frames (short)

- **Embodied cognition / enactivism** â€” cognition arises from sensorimotor engagement with the world; mind and body are inseparable.
    
- **Extended mind / distributed cognition** (Clark & Chalmers; Hutchins) â€” cognition spreads across tools, artifacts, institutions, people. Culture _is_ cognitive machinery.
    
- **Sociocultural scaffold** (Vygotsky-like) â€” learning happens through social mediation; expertise is co-constructed.
    
- **Complementary intelligences** â€” think in terms of ecosystems: different intelligences (human, robotic, statistical, symbolic) each bring niche strengths that together enable capacities none alone could.
    

# Practical reframe for terminology

Instead of â€œAGI,â€ you might hear/use terms like:

- **Artificial Embodied Intelligence (AEI)** â€” highlights body/environment coupling.
    
- **Augmented General Intelligence (AugGI)** â€” emphasizes human+machine hybrid systems that co-imagine and co-perform.
    
- **Socio-Technical Generality** â€” names the social institutions and distributed cognition that make â€œgeneralâ€ performance possible.
    

Each of these reframes reduces the myth of a disembodied, universal intelligence and centers embodiment, culture, or partnership.

# Where this matters (concrete consequences)

- **Design**: Build AIs as collaborators that remember social norms, apologize, defer, ask for help â€” not just because they can say â€œI donâ€™t know,â€ but because they participate in social workflows that enforce caution.
    
- **Evaluation**: Measure success not only by benchmark accuracy but by robustness in embodied / situated tasks, safe uncertainty, cultural competence, and how well they integrate into human teams.
    
- **Governance**: Policies should treat AI as tools embedded in social systems (liability, auditing, training data provenance), not mindless oracles.
    
- **Opportunity**: Lean into the unique strengths of AI (scale, multi-modal pattern detection, continuous monitoring) to augment human practice rather than copy it.
    

# Quick reading list (short, high-signal)

- Andy Clark â€” _Supersizing the Mind_ (extended mind)
    
- Alva NoÃ« / Evan Thompson â€” work on enactivism / embodiment
    
- Edwin Hutchins â€” _Cognition in the Wild_ (distributed cognition)
    
- Rodney Brooks â€” robotics, embodiment arguments
    
- Papers/voices on â€œhuman + AIâ€ collaboration and agents (various recent CS/AI eco literature) â€” if you want, I can pull a short, up-to-date reading list with links.
    

# Want to do something practical next?

I can:

- Draft a short manifesto/paragraph reframing â€œAGIâ€ into a term you like (AEI / AugGI / something you name).
    
- Create a two-column table _right now_ mapping â€œHuman strengthsâ€ vs â€œAI strengthsâ€ vs â€œHybrid possibilitiesâ€ to help you visualize complementary roles.
    
- Pull a short, current reading list and a couple accessible essays (I can fetch up-to-date links if you want me to search the web).
    

Which of those would you like me to do next? Or, if you prefer, I can just draft the two-column table here without web searching.
:::

:::human
This topic space is incredibly rich, which makes sense because we are dreaming the future. So I actually wonder if we can work backwardsâ€¦ science fiction as a genre could be useful in backing into possibilities with imagined concrete examples. I know the future will be a lot weirder than any s.f. author can conceiveâ€¦ but who are some authors exploring this space through fiction, who are relatively up to date? I liked some of Ted Chiangâ€™s and (the author of The Paper Menagerie)â€™s work - although I didnâ€™t resonate much with Chaingâ€™s â€œBlurry JPEGâ€ New Yorker article. Maybe Iâ€™ll explore those limitations in another conversation.
:::

:::ai
Cool idea â€” using science fiction as a looking glass to sketch what might be possible (or necessary). Here are a number of SF authors (and a few works) who feel especially up-to-date, thoughtful, or relevant to the questions youâ€™re raising around embodiment, AI, human/machine difference, social cognition, etc. Iâ€™ll also include a few works that push too far in one direction or another â€” useful for seeing the edges.

---

## ğŸ” Authors & Works to Check Out

|Author / Title|What they explore that resonates with your concerns (embodiment, culture, hybridity, social norms, â€œAI strengthsâ€)|
|---|---|
|**Silvia Park â€” _Luminous_** (2025)|Explores the intersection of human, cyborg, robot; grapples with how identity, grief, augmentation, and relationships are altered when the â€œbodyâ€ boundary starts to blur. ([The Guardian](https://www.theguardian.com/books/2025/apr/23/luminous-by-silvia-park-review-a-major-new-voice-in-sf?utm_source=chatgpt.com "Luminous by Silvia Park review - a major new voice in SF"))|
|**Emma Mieko Candon â€” _The Archive Undying_** (2023)|Deals with AI gods, memory, the idea of â€œrelicsâ€ (humans interfaced with damaged/broken AI), â€œarchivesâ€ (non-sentient AI memories), etc. It addresses what happens when sentience and identity get entangled with the corrupted, dead, human-machine interfaces. ([Wikipedia](https://en.wikipedia.org/wiki/The_Archive_Undying?utm_source=chatgpt.com "The Archive Undying"))|
|**Michael Grothaus â€” _Beautiful Shining People_** (2023)|Near-future world with androids, social media, deepfakes, political coercion; very much about how tech and identity and power interact. Itâ€™s especially strong in its emotional and relational scenes â€” how the tech touches human lives, vulnerabilities. ([Wikipedia](https://en.wikipedia.org/wiki/Beautiful_Shining_People?utm_source=chatgpt.com "Beautiful Shining People"))|
|**Void Star â€” Zachary Mason** (2017)|Explores neural implants (â€œperfect memory recallâ€), identity, virtual reality, imprinting, etc. Some of what youâ€™d want in terms of AI + embodied mind + signal vs. noise. Itâ€™s older, but still valuable. ([Wikipedia](https://en.wikipedia.org/wiki/Void_Star?utm_source=chatgpt.com "Void Star"))|
|**Kazuo Ishiguro â€” _Klara and the Sun_** (2021)|A quieter, more intimate take: what consciousness and emotion look like from a non-human viewpoint; how observation, care, hope, faith, and senses in an artificial being might differ. Offers a grounded lens. ([STARCREST](https://www.paulofurtado.com/viewpoint-7-sci-fi-novels-that-teach-us-about-being-human-in-the-age-of-ai/?utm_source=chatgpt.com "Viewpoint: 7 Sci-Fi Novels That Teach Us About Being Human ..."))|
|**Rie Qudan â€” _Tokyo Sympathy Tower_** (2024)|A recent Japanese novel (winner of an important literary prize) that incorporates AI, near-future settings, architectural imagination, social ethics, etc. Itâ€™s interesting because of both its cultural specificity and how it imagines AI as woven into human projects. ([Wikipedia](https://en.wikipedia.org/wiki/Rie_Qudan?utm_source=chatgpt.com "Rie Qudan"))|

---

## ğŸ”® Other Authors worth exploring

These may not hit _all_ your marks, but they have elements youâ€™ll probably appreciate:

- Annalee Newitz â€” _Autonomous_ â€” biotech, AI, moral complexity. ([Celadon Books](https://celadonbooks.com/booklists/novels-about-artificial-intelligence/?utm_source=chatgpt.com "The Future Is Now: Novels About Artificial Intelligence"))
    
- N. K. Jemisin â€” often writes about social systems, identity, power, otherness; sometimes with speculative tech.
    
- Neal Stephenson â€” for big systems thinking, especially when tech + culture + economics collide.
    
- Rebecca Roanhorse â€” for cultural specificity + speculative/post-apocalyptic landscapes.
    

---

## âš ï¸ Where the limits or tensions show up (things you might _not_ like or want to question)

- Many SF works still lean on tropes of â€œAI as mimic/humanness vs AI as threatâ€ rather than exploring AIâ€™s â€œotherâ€ affordances.
    
- Some dehumanize (or over-alienate) AI characters so that their â€œdifferenceâ€ becomes spectacle rather than a lens for mutuality or hybridity.
    
- Others default to dystopia, which can be useful but sometimes misses more nuanced imaginaries of partnership, co-imagination, or complementarity.
    

---

If you want, I can try to pull together a **short curated list** of SF stories (novellas / short stories) that explicitly imagine futures where intelligence is radically different (embodied differently) â€” maybe ones that explore â€œnon-human cognitionâ€ (insects, collective intelligences, AI with different sensory modalities, etc.). Would you prefer that or maybe an annotated list in order of â€œmost aligned with your sense of whatâ€™s possible / inspiringâ€?
:::

:::human
_Tokyo Sympathy Tower_ gave me some prickles, above. If Iâ€™m trusting my body, I suppose thatâ€™s where Iâ€™d start. I think Iâ€™m less interested in the wild sensory modalities than in exploring positive cultural possibilitiesâ€¦ but that are grounded, as well. I wanted to be affected/changed by what I read: ideas which expand my mind and conceptions around whatâ€™s possible, through some kind of mysterious limbic resonance with the authorâ€¦ thatâ€™s why I liked some of the stories in _The Paper Menagerie_. There were affectiveâ€¦ I learned about my/this reality, in a deeper way.
:::

:::ai
Lovely â€” I hear you: you want fiction that _moves_ you, thatâ€™s emotionally true and culturally grounded, and that expands what you imagine is possible without sliding into spectacle or cold futurism. Below is a compact, hand-picked reading list (short â†’ longer) with one-line why each choice might give you that limbic resonance you described.

# Start here â€” short, high-impact pieces

1. **Ken Liu â€” â€œThe Paper Menagerieâ€**  
    â†’ Pure affect: family, memory, cultural belonging; a small story that can change how you feel about language and intimacy.
    
2. **Ken Liu â€” â€œMono no awareâ€**  
    â†’ Quiet, humane, a moral/affective take on contact and consequence; works the heart and the imagination.
    
3. **Ted Chiang â€” â€œThe Story of Your Lifeâ€**  
    â†’ Big idea + tenderness; shows how a concept (language/time) can be lived emotionally.
    
4. **Aliette de Bodard â€” â€œThe Tea Master and the Detectiveâ€** (novella)  
    â†’ Intimate, elegiac, and culturally textured â€” a detective story where the â€œmindâ€ is not Western rationality but relational ships & communities.
    
5. **Nnedi Okorafor â€” â€œBintiâ€** (novella)  
    â†’ Identity, tradition, and transformation â€” spare, sung, and deeply human.
    

# Mid-length / novels that stay grounded and humane

6. **Becky Chambers â€” _A Closed and Common Orbit_** (Wayfarers series)  
    â†’ Optimistic, character-driven SF about belonging, repair, and community â€” the kind of future that feels like a useful model for real cultural possibility.
    
7. **Kazuo Ishiguro â€” _Klara and the Sun_**  
    â†’ Quiet, intimate exploration of artificial caretakers, love, and what personhood looks like in a socially scaffolded world.
    

# For textures of culture + sociopolitical imagination

8. **Michael Grothaus â€” _Beautiful Shining People_**  
    â†’ Near-future critique of tech + power with real emotional beats about how tech rearranges human vulnerability.
    
9. **N. K. Jemisin â€” _The City Born Great_ (short/novella)**  
    â†’ Fierce, urban, and collective â€” not techy in the classic sense but brilliant at imagining civic intelligence and cultural agency.
    

---

If you want a reading _order_ that builds affectively: start with a Ken Liu short, then Ted Chiangâ€™s story, then Aliette de Bodardâ€™s novella, then Becky Chambers, then Ishiguro. That progression moves from immediate emotional shock â†’ conceptual tenderness â†’ relational spec fic â†’ full-bodied, humane novel.

Would you like me to:

- make this into a one-page reading plan (2â€“3 weeks, 30â€“60 mins/day) with short prompts so each piece lands more deeply?
    
- or assemble 8â€“10 short stories (all under 20 pages) for a weekend of limbic resonance?
    

Tell me which and Iâ€™ll lay it out.
:::